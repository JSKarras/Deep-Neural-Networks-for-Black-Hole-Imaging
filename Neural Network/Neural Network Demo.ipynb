{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Method for Black Hole Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to eht-imaging! v  1.1.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from numpy.random import randint\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tf.keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import RandomUniform, Constant\n",
    "import keras.models\n",
    "from keras.models import Sequential\n",
    "import keras.layers\n",
    "from keras.layers import Layer, Activation, LeakyReLU\n",
    "from keras.layers import Input, InputLayer, AveragePooling2D, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate, Add\n",
    "from keras.layers import Dense, Lambda, Reshape\n",
    "import keras.initializers\n",
    "import keras.regularizers\n",
    "import keras.callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Import eht imaging package\n",
    "import ehtim as eh \n",
    "import ehtim.const_def as ehc\n",
    "import ehtim.observing.obs_helpers as obsh\n",
    "from ehtim.observing.obs_helpers import *\n",
    "# Import helpers from cosense\n",
    "#import helpers_posci as hp\n",
    "# Import utilities for computing data terms, losses, and gradients\n",
    "#from data_term_functions import *\n",
    "##from models_posci import IsingVisNet, IsingCpAmpNet, IsingMutipleVisNet, IsingMutipleCpAmpNet, IsingVisFeatureNet, IsingCpAmpFeatureNet\n",
    "#from losses_posci import site_sparsity, energy, Lambda_similarity, Lambda_angle_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Imaging Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define observation parameters.'''\n",
    "eht_array='EHT2019'\n",
    "target='sgrA'\n",
    "\n",
    "nsamp = 10000\n",
    "npix = 32 \n",
    "fov_param = 100.0\n",
    "flux_label = 1\n",
    "sefd_param = 1\n",
    "\n",
    "tint_sec = 5    # integration time in seconds\n",
    "tadv_sec = 600  # advance time between scans\n",
    "tstart_hr = 0   # GMST time of the start of the observation\n",
    "tstop_hr = 24   # GMST time of the end\n",
    "bw_hz = 4e9     # bandwidth in Hz\n",
    "\n",
    "stabilize_scan_phase = False # if true then add a single phase error for each scan to act similar to adhoc phasing\n",
    "stabilize_scan_amp = False # if true then add a single gain error at each scan\n",
    "jones = False # apply jones matrix for including noise in the measurements (including leakage)\n",
    "inv_jones = False # no not invert the jones matrix\n",
    "frcal = True # True if you do not include effects of field rotation\n",
    "dcal = True # True if you do not include the effects of leakage\n",
    "dterm_offset = 0 # a random offset of the D terms is given at each site with this standard deviation away from 1\n",
    "dtermp = 0\n",
    "\n",
    "array = '/Users/Johanna/Desktop/Proximal Gradient Descent/arrays/' + eht_array + '.txt'\n",
    "eht = eh.array.load_txt(array)\n",
    "\n",
    "# Define observation field of view\n",
    "fov = fov_param * eh.RADPERUAS\n",
    "\n",
    "# define scientific target\n",
    "if target == 'm87':\n",
    "    ra = 12.513728717168174\n",
    "    dec = 12.39112323919932\n",
    "elif target == 'sgrA':\n",
    "    ra = 19.414182210498385\n",
    "    dec = -29.24170032236311\n",
    "\n",
    "rf = 230e9\n",
    "mjd = 57853 # day of observation\n",
    "fwhm = 1.117609542559987e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Prepare and return dataset for RML. Fashion MNIST and Digits MNIST images are blurred by 0.1*fwhm.\n",
    "    \n",
    "    -----------------------------------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "        -dataset: 'fashion' (fashion MNIST, 'mnist' (MNIST digits), or 'bh_data' (simulated black hole images)\n",
    "        -flux: sum of pixels per image\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "'''\n",
    "def get_data(dataset='fashion', flux=1):\n",
    "    xdata = []\n",
    "    pad_width = 2\n",
    "    if (dataset == 'fashion' or dataset == 'all'):\n",
    "        npix = 32\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "        xdata = 1.0*x_train[[k%60000 for k in range(int(nsamp))]]\n",
    "        xdata = np.pad(xdata, ((0,0), (pad_width,pad_width), (pad_width,pad_width)), 'constant')\n",
    "        xdata = xdata[..., np.newaxis]/255\n",
    "        \n",
    "        xdata = xdata.reshape((-1, npix*npix))\n",
    "        \n",
    "        # Blur images by 0.1*fwhm\n",
    "        xdata_blur = []\n",
    "        for X in xdata:\n",
    "            pred_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "            pred_blur.imvec = X.flatten()\n",
    "            pred_blur = pred_blur.blur_circ(fwhm_i=0.1*fwhm, fwhm_pol=0.1*fwhm)\n",
    "            xdata_blur.append(pred_blur.imvec)\n",
    "        xdata = xdata_blur\n",
    "            \n",
    "    if (dataset == 'mnist' or dataset == 'all'):\n",
    "        npix = 32\n",
    "        (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "        \n",
    "        xdata_train = 1.0*x_train_mnist[[k%60000 for k in range(int(nsamp))]]\n",
    "        xdata_train = np.pad(xdata_train, ((0,0), (pad_width,pad_width), (pad_width,pad_width)), 'constant')  # get to 160x160\n",
    "        xdata_train = xdata_train[..., np.newaxis]/255\n",
    "        \n",
    "        xdata = xdata_train.reshape((-1, npix*npix))\n",
    "        \n",
    "        # Blur images by 0.1*fwhm\n",
    "        xdata_blur = []\n",
    "        for X in xdata:\n",
    "            pred_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "            pred_blur.imvec = X.flatten()\n",
    "            pred_blur = pred_blur.blur_circ(fwhm_i=0.1*fwhm, fwhm_pol=0.1*fwhm)\n",
    "            xdata_blur.append(pred_blur.imvec)\n",
    "        xdata = xdata_blur\n",
    "        \n",
    "    if (dataset == 'bh_data'):\n",
    "        bh_sim_data = np.load('/Users/Johanna/Desktop/Neural Network/bh_sim_data.npy', allow_pickle=True).item()\n",
    "        bh_data = bh_sim_data['image']\n",
    "        \n",
    "        # resize images to 32 x 32 and fov = 100\n",
    "        bh_data = np.array(bh_data)\n",
    "        bh_data_reshape = []\n",
    "        for i in range(len(bh_data)):\n",
    "            bh_img = eh.image.make_empty(160, 160, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "            bh_img.imvec = bh_data[i].flatten()\n",
    "            bh_img_reshape = bh_img.regrid_image(100, 32)\n",
    "            bh_data_reshape.append(bh_img_reshape.imvec)\n",
    "        xdata = np.array(bh_data_reshape).reshape((-1, 32*32))\n",
    "    \n",
    "    return xdata\n",
    "\n",
    "'''\n",
    "    Generate measurement data for image X using specified parameters. \n",
    "    \n",
    "    -----------------------------------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "        -X: target image\n",
    "        -th_noise: If True, include thermal noise in measurements\n",
    "        -amp_error: If True, include amplitude error in measurements\n",
    "        -phase_error: If True, include phase error in measurements\n",
    "        -gainp: Amount of site-wise standard deviation in gain error to include in measurements\n",
    "        -gain_offset: Amount of fixed gain error to include in measurements\n",
    "    -----------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "def get_measurements(X, th_noise=False, amp_error=False, phase_error=False, gainp=0.1, gain_offset=0.1):\n",
    "    # Define noise parameters\n",
    "    add_th_noise = th_noise # False if you *don't* want to add thermal error. If there are no sefds in obs_orig it will use the sigma for each data point\n",
    "    phasecal = not phase_error # True if you don't want to add atmospheric phase error. if False then it adds random phases to simulate atmosphere\n",
    "    ampcal = not amp_error # True if you don't want to add atmospheric amplitude error. if False then add random gain errors \n",
    "\n",
    "    simim = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    simim.imvec = X\n",
    "    \n",
    "    # generate the discrete Fourier transform matrices for complex visibilities\n",
    "    obs = simim.observe(eht, tint_sec, tadv_sec, tstart_hr, tstop_hr, bw_hz, add_th_noise=add_th_noise, ampcal=ampcal, phasecal=phasecal, \n",
    "                    stabilize_scan_phase=stabilize_scan_phase, stabilize_scan_amp=stabilize_scan_amp,\n",
    "                    jones=jones,inv_jones=inv_jones,dcal=dcal, frcal=frcal, dterm_offset=dterm_offset, \n",
    "                    gainp=gainp, gain_offset=gain_offset)\n",
    "    obs_data = obs.unpack(['u', 'v', 'vis', 'sigma'])\n",
    "    \n",
    "    uv = np.hstack((obs_data['u'].reshape(-1,1), obs_data['v'].reshape(-1,1)))\n",
    "    \n",
    "    # Extract forward model (Discrete Fourier Transform matrix)\n",
    "    F_vis = ftmatrix(simim.psize, simim.xdim, simim.ydim, uv, pulse=simim.pulse)\n",
    "    vis = obs_data['vis']\n",
    "    sigma_vis = obs_data['sigma']\n",
    "    t1 = obs.data['t1']\n",
    "    t2 = obs.data['t2']\n",
    "    \n",
    "    print(\"Finished computing visibilities...\")\n",
    "    \n",
    "    # generate the discrete Fourier transform matrices for closure phases\n",
    "    obs.add_cphase(count='max')\n",
    "    # Extract forward models for telescopes 1, 2, and 3\n",
    "    tc1 = obs.cphase['t1']\n",
    "    tc2 = obs.cphase['t2']\n",
    "    tc3 = obs.cphase['t3']\n",
    "    \n",
    "    cphase = obs.cphase['cphase']\n",
    "    sigma_cphase = obs.cphase['sigmacp']\n",
    "    cphase_map = np.zeros((len(obs.cphase['time']), 3))\n",
    "\n",
    "    zero_symbol = 10000\n",
    "    for k1 in range(cphase_map.shape[0]):\n",
    "        for k2 in list(np.where(obs.data['time']==obs.cphase['time'][k1])[0]):\n",
    "            if obs.data['t1'][k2] == obs.cphase['t1'][k1] and obs.data['t2'][k2] == obs.cphase['t2'][k1]:\n",
    "                cphase_map[k1, 0] = k2\n",
    "                if k2 == 0:\n",
    "                    cphase_map[k1, 0] = zero_symbol\n",
    "            elif obs.data['t2'][k2] == obs.cphase['t1'][k1] and obs.data['t1'][k2] == obs.cphase['t2'][k1]:\n",
    "                cphase_map[k1, 0] = -k2\n",
    "                if k2 == 0:\n",
    "                    cphase_map[k1, 0] = -zero_symbol\n",
    "            elif obs.data['t1'][k2] == obs.cphase['t2'][k1] and obs.data['t2'][k2] == obs.cphase['t3'][k1]:\n",
    "                cphase_map[k1, 1] = k2\n",
    "                if k2 == 0:\n",
    "                    cphase_map[k1, 1] = zero_symbol\n",
    "            elif obs.data['t2'][k2] == obs.cphase['t2'][k1] and obs.data['t1'][k2] == obs.cphase['t3'][k1]:\n",
    "                cphase_map[k1, 1] = -k2\n",
    "                if k2 == 0:\n",
    "                    cphase_map[k1, 1] = -zero_symbol\n",
    "            elif obs.data['t1'][k2] == obs.cphase['t3'][k1] and obs.data['t2'][k2] == obs.cphase['t1'][k1]:\n",
    "                cphase_map[k1, 2] = k2\n",
    "                if k2 == 0:\n",
    "                    cphase_map[k1, 2] = zero_symbol\n",
    "            elif obs.data['t2'][k2] == obs.cphase['t3'][k1] and obs.data['t1'][k2] == obs.cphase['t1'][k1]:\n",
    "                cphase_map[k1, 2] = -k2\n",
    "                if k2 == 0:\n",
    "                    cphase_map[k1, 2] = -zero_symbol\n",
    "\n",
    "    F_cphase = np.zeros((cphase_map.shape[0], npix*npix, 3), dtype=np.complex64)\n",
    "    cphase_proj = np.zeros((cphase_map.shape[0], F_vis.shape[0]), dtype=np.float32)\n",
    "    for k in range(cphase_map.shape[0]):\n",
    "        for j in range(cphase_map.shape[1]):\n",
    "            if cphase_map[k][j] > 0:\n",
    "                if int(cphase_map[k][j]) == zero_symbol:\n",
    "                    cphase_map[k][j] = 0\n",
    "                F_cphase[k, :, j] = F_vis[int(cphase_map[k][j]), :]\n",
    "                cphase_proj[k, int(cphase_map[k][j])] = 1\n",
    "            else:\n",
    "                if np.abs(int(cphase_map[k][j])) == zero_symbol:\n",
    "                    cphase_map[k][j] = 0\n",
    "                F_cphase[k, :, j] = np.conj(F_vis[int(-cphase_map[k][j]), :])\n",
    "                cphase_proj[k, int(-cphase_map[k][j])] = -1\n",
    "\n",
    "    clamparr = obs.c_amplitudes(mode='all', count='max',\n",
    "                                        vtype='vis', ctype='camp', debias=True, snrcut=0.0)\n",
    "    \n",
    "\n",
    "    uv1 = np.hstack((clamparr['u1'].reshape(-1, 1), clamparr['v1'].reshape(-1, 1)))\n",
    "    uv2 = np.hstack((clamparr['u2'].reshape(-1, 1), clamparr['v2'].reshape(-1, 1)))\n",
    "    uv3 = np.hstack((clamparr['u3'].reshape(-1, 1), clamparr['v3'].reshape(-1, 1)))\n",
    "    uv4 = np.hstack((clamparr['u4'].reshape(-1, 1), clamparr['v4'].reshape(-1, 1)))\n",
    "    camp = clamparr['camp']\n",
    "    sigma_camp = clamparr['sigmaca']\n",
    "    \n",
    "    mask = []\n",
    "    # shape: (4, 2022, npix**2)\n",
    "    F_camp = (ftmatrix(simim.psize, simim.xdim, simim.ydim, uv1, pulse=simim.pulse, mask=mask),\n",
    "          ftmatrix(simim.psize, simim.xdim, simim.ydim, uv2, pulse=simim.pulse, mask=mask),\n",
    "          ftmatrix(simim.psize, simim.xdim, simim.ydim, uv3, pulse=simim.pulse, mask=mask),\n",
    "          ftmatrix(simim.psize, simim.xdim, simim.ydim, uv4, pulse=simim.pulse, mask=mask)\n",
    "          )\n",
    "    \n",
    "    return obs, vis, cphase, camp, F_vis, F_cphase, F_camp, sigma_vis, sigma_cphase, sigma_camp, t1, t2\n",
    "\n",
    "# Split into Training and Testing Sets (Complex Visibilities)\n",
    "def split_data(X, vis):\n",
    "    split = int(0.9*len(xdata))\n",
    "    X_train = vis[:split]\n",
    "    y_train = xdata[:split].reshape((len(X_train), npix, npix, 1))\n",
    "    X_test = vis[split:]\n",
    "    y_test = xdata[split:].reshape((len(X_test), npix, npix, 1))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Post Processing\n",
    "def post_process(Z, img):\n",
    "    flux, X_max = np.sum(img), np.max(img)\n",
    "\n",
    "    # Normalize flux to target image\n",
    "    Z_flux = np.sum(np.abs(Z))\n",
    "    Z = (flux/Z_flux)*np.abs(Z)\n",
    "\n",
    "    # Normalize Z between 0 and X_max\n",
    "    Z = np.maximum(np.zeros(np.shape(Z)), Z)\n",
    "    Z = np.minimum(X_max*np.ones(np.shape(Z)), Z)\n",
    "    \n",
    "    return Z\n",
    "\n",
    "# Compute normalized cross-correlation between images X and Z\n",
    "def compute_xcorr(X, Z):\n",
    "    target_img = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    target_img.imvec = X.flatten()\n",
    "    recon_img = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    recon_img.imvec = Z.flatten()\n",
    "    xc = target_img.compare_images(recon_img)[0][0] \n",
    "    return xc\n",
    "    \n",
    "# Compute MAE loss \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def compute_mae(X, Z):\n",
    "    mae = mean_absolute_error(X.flatten(), Z.flatten())\n",
    "    return mae\n",
    "\n",
    "# Compute SSIM\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "def compute_ssim(X, Z):\n",
    "    return ssim(X.flatten(), Z.flatten())\n",
    "    \n",
    "''' Show results '''\n",
    "def visualize(result, obs):\n",
    "    # Unpack values\n",
    "    [target, pred, uncertainty, error, vis_chisq, cphase_chisq, camp_chisq, mae, ssim] = result\n",
    "    \n",
    "    max_color = 0.01\n",
    "    if dataset == 'fashion':\n",
    "        max_color = 0.005\n",
    "    \n",
    "    # Compute nominally blurred target image\n",
    "    fwhm = obs.res()\n",
    "    target_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    target_blur.imvec = target.flatten()\n",
    "    target_blur1 = target_blur.blur_circ(fwhm_i=0.3*fwhm, fwhm_pol=0.3*fwhm)\n",
    "    target_blur2 = target_blur.blur_circ(fwhm_i=0.7*fwhm, fwhm_pol=0.7*fwhm)\n",
    "    \n",
    "    # Show nominally blurred target image\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(figsize=(13, 3), ncols=3)\n",
    "\n",
    "    ground_truth = ax1.imshow((target).reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax1.title.set_text('Ground Truth')\n",
    "    fig.colorbar(ground_truth, ax=ax1)\n",
    "    blur1 = ax2.imshow(target_blur1.imvec.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax2.title.set_text('0.3 * fwhm Blurred Truth')\n",
    "    fig.colorbar(blur1, ax=ax2)\n",
    "    blur2 = ax3.imshow(target_blur2.imvec.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax3.title.set_text('0.7 * fwhm Blurred Truth')\n",
    "    cbar = fig.colorbar(blur2, ax=ax3)\n",
    "    cbar.minorticks_on()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute nominally blurred predicted image\n",
    "    fwhm = obs.res()\n",
    "    pred_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    pred_blur.imvec = pred.flatten()\n",
    "    pred_blur1 = pred_blur.blur_circ(fwhm_i=0.3*fwhm, fwhm_pol=0.3*fwhm)\n",
    "    pred_blur2 = pred_blur.blur_circ(fwhm_i=0.7*fwhm, fwhm_pol=0.7*fwhm)\n",
    "    \n",
    "    # Visualize blurred results\n",
    "    fig3, (ax7, ax8, ax9) = plt.subplots(figsize=(13, 3), ncols=3)\n",
    "\n",
    "    pred_img = ax7.imshow(pred.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax7.title.set_text('Predicted Image')\n",
    "    fig3.colorbar(pred_img, ax=ax7)\n",
    "    blur1_img = ax8.imshow(pred_blur1.imvec.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax8.title.set_text('0.3 * fwhm blur')\n",
    "    fig3.colorbar(blur1_img, ax=ax8)\n",
    "    blur2_img = ax9.imshow(pred_blur2.imvec.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax9.title.set_text('0.7 * fwhm blur')\n",
    "    fig3.colorbar(blur2_img, ax=ax9)\n",
    "    cbar.minorticks_on()\n",
    "    plt.show()\n",
    "           \n",
    "    # Visualize uncertainty\n",
    "    fig2, (ax4, ax5, ax6) = plt.subplots(figsize=(13, 3), ncols=3)\n",
    "\n",
    "    pred_img = ax4.imshow(pred.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax4.title.set_text('Predicted Image')\n",
    "    fig2.colorbar(pred_img, ax=ax4)\n",
    "    uncertainty_img = ax5.imshow(uncertainty.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax5.title.set_text('Pixel-Wise Uncertainty')\n",
    "    fig2.colorbar(uncertainty_img, ax=ax5)\n",
    "    error_img = ax6.imshow(error.reshape(npix, npix), vmin=0, vmax=max_color)\n",
    "    ax6.title.set_text('Absolute Error')\n",
    "    fig2.colorbar(error_img, ax=ax6)\n",
    "    cbar.minorticks_on()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"Vis Chi^2 = \", round(vis_chisq, 10))\n",
    "    #print(\"Cphase Chi^2 = \", round(cphase_chisq, 4))\n",
    "    #print(\"Camp Chi^2 = \", round(camp_chisq, 4))\n",
    "    print(\"MAE = \", mae)\n",
    "    print(\"SSIM = \", ssim)\n",
    "\n",
    "    print()\n",
    "\n",
    "''' Get resolution of reconstructed image: blurred target image versus predicted image. '''\n",
    "def get_res(recon, target, obs, simim1, simim2):\n",
    "    # Create target and recon image objects\n",
    "    target_img = simim1\n",
    "    target_img.imvec = target.flatten()\n",
    "    recon_img = simim2\n",
    "    recon_img.imvec = recon.flatten()\n",
    "        \n",
    "    # Compute cross-correlation b/w recon and target images\n",
    "    recon_xc = target_img.compare_images(recon_img)[0][0]\n",
    "    \n",
    "    alphas = np.linspace(0, 50, 1000)\n",
    "    xcorr = []\n",
    "    for alpha in alphas:\n",
    "        target_blur = target_img.blur_circ(alpha*eh.RADPERUAS) # Add gaussian blur in micro-arcsecs\n",
    "        xc = target_blur.compare_images(recon_img)[0][0] # Get normalized cross-correlation\n",
    "        xcorr.append(xc)\n",
    "            \n",
    "    # Get nominal resolution\n",
    "    nominal_res = obs.res()\n",
    "    \n",
    "    # Get image resolution\n",
    "    recon_alpha = np.array(xcorr).argmax()*50/1000 # Get index of blurring parameter with highest xcorr\n",
    "    \n",
    "    # Show Plot\n",
    "    plt.figure()\n",
    "    plt.plot(alphas, xcorr, label=\"XCorr\")\n",
    "    plt.axvline(x=nominal_res/eh.RADPERUAS, label = \"Nominal Resolution (uas)\", color='m')\n",
    "    if recon_alpha is not None:\n",
    "        plt.axvline(x=recon_alpha, label = \"Reconstruction Resolution (uas)\", color='g')\n",
    "    plt.ylim(0.6, 1)\n",
    "    plt.xlim(0, 50)\n",
    "    plt.xlabel(\"Blurring Parameter, alpha (uas)\")\n",
    "    plt.ylabel(\"Normalized XCorr\")\n",
    "    plt.title(\"XCorr of Blurred Target/Reconstruction vs. Alpha\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Reconstruction XC = \", recon_xc)\n",
    "    print(\"Reconstruction Alpha = \", recon_alpha)\n",
    "    \n",
    "    return recon_xc\n",
    "\n",
    "def compute_snr(X, sigma):\n",
    "    # Define noise parameters\n",
    "    add_th_noise = False # False if you *don't* want to add thermal error. If there are no sefds in obs_orig it will use the sigma for each data point\n",
    "    phasecal = True # True if you don't want to add atmospheric phase error. if False then it adds random phases to simulate atmosphere\n",
    "    ampcal = True # True if you don't want to add atmospheric amplitude error. if False then add random gain errors \n",
    "\n",
    "    simim = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    simim.imvec = X\n",
    "    \n",
    "    # generate the discrete Fourier transform matrices for complex visibilities\n",
    "    obs = simim.observe(eht, tint_sec, tadv_sec, tstart_hr, tstop_hr, bw_hz, add_th_noise=add_th_noise, ampcal=ampcal, phasecal=phasecal, \n",
    "                    stabilize_scan_phase=stabilize_scan_phase, stabilize_scan_amp=stabilize_scan_amp,\n",
    "                    jones=jones,inv_jones=inv_jones,dcal=dcal, frcal=frcal, dterm_offset=dterm_offset)\n",
    "    obs_data = obs.unpack(['u', 'v', 'vis', 'sigma'])\n",
    "    \n",
    "    vis = obs_data['vis']\n",
    "    \n",
    "    snr = np.mean(vis**2) / np.mean(sigma**2)\n",
    "    \n",
    "    return snr\n",
    "\n",
    "def chisq_loss(x_true0, x_pred0):\n",
    "    F = tf.cast(tf.constant(global_F), tf.complex64)\n",
    "    S = tf.cast(tf.constant(global_S), tf.float32)\n",
    "    \n",
    "    # Flatten and normalize image arrays\n",
    "    x_true0 = tf.cast(tf.reshape(x_true0, [-1, 1024]), tf.complex64)\n",
    "    x_pred0 = tf.cast(tf.reshape(x_pred0, [-1, 1024]), tf.complex64)\n",
    "    x_true0 = tf.math.divide(x_true0, [500])#tf.transpose(tf.cast(tf.reduce_sum(x_true0, 1), tf.complex64)))\n",
    "    x_pred0 = tf.math.divide(x_pred0, [500])#tf.transpose(tf.cast(tf.reduce_sum(x_pred0, 1), tf.complex64)))\n",
    "\n",
    "    # Compute visibilities\n",
    "    vis_true = tf.matmul(x_true0, tf.transpose(F))\n",
    "    vis_pred = tf.matmul(x_pred0, tf.transpose(F))\n",
    "\n",
    "    # numerator\n",
    "    num = tf.reduce_sum(tf.square(tf.divide(tf.abs(tf.subtract(vis_pred, vis_true)), S[:,0])))\n",
    "    \n",
    "    chisq = tf.divide(num, tf.cast(tf.multiply(2, tf.size(vis_true[0])), tf.float32))\n",
    "    \n",
    "    return chisq\n",
    "\n",
    "def plotall(obs, field1, field2, conj=False, debias=True, tag_bl=False, \n",
    "            ang_unit='deg', timetype=False,  axis=False, rangex=False, \n",
    "            rangey=False, snrcut=0., color='b', marker='o', \n",
    "            markersize=ehc.MARKERSIZE, label=None, grid=True, ebar=True, \n",
    "            axislabels=True, legend=False, show=False):\n",
    "    bllist = [['All', 'All']]\n",
    "    colors = ehc.SCOLORS[0]\n",
    "\n",
    "    # unpack data\n",
    "    alldata = [obs.unpack([field1, field2],\n",
    "                           conj=conj, ang_unit=ang_unit, debias=debias, timetype=timetype)]\n",
    "\n",
    "    # X error bars\n",
    "    if obsh.sigtype(field1):\n",
    "        allsigx = obs.unpack(obsh.sigtype(field2), conj=conj, ang_unit=ang_unit)\n",
    "        allsigx = [allsigx[obsh.sigtype(field1)]]\n",
    "    else:\n",
    "        allsigx = [None]\n",
    "\n",
    "    # Y error bars\n",
    "    if obsh.sigtype(field2):\n",
    "        allsigy = obs.unpack(obsh.sigtype(field2), conj=conj, ang_unit=ang_unit)\n",
    "        allsigy = [allsigy[obsh.sigtype(field2)]]\n",
    "        \n",
    "    else:\n",
    "        allsigy = [None]\n",
    "            \n",
    "    # make plot(s)\n",
    "    if axis:\n",
    "        x = axis\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "        x = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    xmins = []\n",
    "    xmaxes = []\n",
    "    ymins = []\n",
    "    ymaxes = []\n",
    "    for i in range(len(alldata)):\n",
    "        data = alldata[i]\n",
    "        sigy = allsigy[i]\n",
    "        sigx = allsigx[i]\n",
    "        color = colors[i]\n",
    "        bl = bllist[i]\n",
    "\n",
    "        # Flag out nans (to avoid problems determining plotting limits)\n",
    "        mask = ~(np.isnan(data[field1]) + np.isnan(data[field2]))\n",
    "\n",
    "        # Flag out due to snrcut\n",
    "        if snrcut > 0.:\n",
    "            sigs = [sigx, sigy]\n",
    "            for jj, field in enumerate([field1, field2]):\n",
    "                if field in ehc.FIELDS_AMPS:\n",
    "                    fmask = data[field] / sigs[jj] > snrcut\n",
    "                elif field in ehc.FIELDS_PHASE:\n",
    "                    fmask = sigs[jj] < (180. / np.pi / snrcut)\n",
    "                elif field in ehc.FIELDS_SNRS:\n",
    "                    fmask = data[field] > snrcut\n",
    "                else:\n",
    "                    fmask = np.ones(mask.shape).astype(bool)\n",
    "                mask *= fmask\n",
    "\n",
    "        data = data[mask]\n",
    "        if sigy is not None:\n",
    "            sigy = sigy[mask]\n",
    "        if sigx is not None:\n",
    "            sigx = sigx[mask]\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        xmins.append(np.min(data[field1]))\n",
    "        xmaxes.append(np.max(data[field1]))\n",
    "        ymins.append(np.min(data[field2]))\n",
    "        ymaxes.append(np.max(data[field2]))\n",
    "        \n",
    "        tolerance = len(data[field2])\n",
    "        \n",
    "        if ebar and (np.any(sigy) or np.any(sigx)):\n",
    "            print(\"sigx = \", sigx)\n",
    "            print(\"sigy = \", sigy)\n",
    "            x.errorbar(data[field1], data[field2], xerr=sigx, yerr=sigy, label='',\n",
    "                        fmt=marker, markersize=markersize, picker=tolerance)\n",
    "        else:\n",
    "            x.plot(data[field1], data[field2], marker, markersize=markersize, picker=tolerance)\n",
    "\n",
    "                \n",
    "        # Plot the data\n",
    "        tolerance = len(data[field2])\n",
    "\n",
    "        x.plot(data[field1], data[field2], marker, markersize=markersize)\n",
    "        \n",
    "    # Data ranges\n",
    "    if not rangex:\n",
    "        rangex = [np.min(xmins) - 0.2 * np.abs(np.min(xmins)),\n",
    "                  np.max(xmaxes) + 0.2 * np.abs(np.max(xmaxes))]\n",
    "        if np.any(np.isnan(np.array(rangex))):\n",
    "            print(\"Warning: NaN in data x range: specifying rangex to default\")\n",
    "            rangex = [-100, 100]\n",
    "\n",
    "    if not rangey:\n",
    "        rangey = [np.min(ymins) - 0.2 * np.abs(np.min(ymins)),\n",
    "                  np.max(ymaxes) + 0.2 * np.abs(np.max(ymaxes))]\n",
    "        if np.any(np.isnan(np.array(rangey))):\n",
    "            print(\"Warning: NaN in data y range: specifying rangey to default\")\n",
    "            rangey = [-100, 100]\n",
    "\n",
    "    x.set_xlim(rangex)\n",
    "    x.set_ylim(rangey)\n",
    "    x.set_xlabel(field1)\n",
    "    x.set_ylabel(field2)\n",
    "    x.set_title(field1 + \" versus \" + field2)\n",
    "    x.grid\n",
    "    #plt.legend()\n",
    "    \n",
    "    #if show:\n",
    "        #plt.show(block=False)\n",
    "    \n",
    "    return x, data[field1], data[field2]\n",
    "    \n",
    "''' Predict with Model and Compute Metrics '''\n",
    "def predict_and_run_test(model, target, ALPHA, th_noise, amp_err, phase_err, gainp=0.1, gain_offset=0.1, th_noise_factor=0, blur_param=0, savefile=None, pred_img=None):\n",
    "    # Normalize fluxes\n",
    "    pred_img = pred_img/np.sum(pred_img)\n",
    "    target /= np.sum(target)\n",
    "        \n",
    "    if th_noise_factor == 0:\n",
    "        th_noise = False\n",
    "       \n",
    "    obs, visibility, cphase, camp, F_vis, F_cphase, F_camp, sigma_vis, sigma_cphase, sigma_camp, t1, t2 = get_measurements(target.flatten(), th_noise, amp_err, phase_err, gainp, gain_offset)  # data terms\n",
    "    \n",
    "    # Blur target to 0.3*fwhm\n",
    "    fwhm = obs.res()\n",
    "    target_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    target_blur.imvec = target.flatten()\n",
    "    target_blur = target_blur.blur_circ(fwhm_i=0.3*fwhm, fwhm_pol=0.3*fwhm)\n",
    "    target = target_blur.imvec\n",
    "        \n",
    "    obs, visibility, cphase, camp, F_vis, F_cphase, F_camp, sigma_vis, sigma_cphase, sigma_camp, t1, t2 = get_measurements(target.flatten(), th_noise, amp_err, phase_err, gainp, gain_offset)  # data terms\n",
    "    \n",
    "    if savefile is not None:\n",
    "        obs.save_txt(savefile+'_obs')\n",
    "    \n",
    "    # add additional layers  of thermal noise\n",
    "    if th_noise_factor > 0:\n",
    "        noise_arr = [th_noise_factor*np.random.normal(0, sigma) for sigma in sigma_vis]\n",
    "        visibility += noise_arr\n",
    "        sigma_vis *= th_noise_factor\n",
    "        print(\"Adding additional thermal noise.... mean = \", (th_noise_factor + 1)*np.mean(np.abs(noise_arr)))\n",
    "    \n",
    "    print(\"SNR = \", compute_snr(target, sigma_vis))\n",
    "    \n",
    "    # Pre-process data for UNet\n",
    "    #target *= ALPHA\n",
    "    visibility *= ALPHA\n",
    "    sigma_vis *= ALPHA\n",
    "\n",
    "    recon_imgs = []\n",
    "    num_tests = 16\n",
    "    for n in range(num_tests):   \n",
    "        img = model.predict(visibility.reshape((-1, visibility.shape[0])))\n",
    "        recon_imgs.append(img)\n",
    "\n",
    "    imgs = np.array(recon_imgs)\n",
    "    pred = np.mean(imgs, axis=0)\n",
    "\n",
    "    # Post-process\n",
    "    visibility /= ALPHA\n",
    "    imgs /= ALPHA\n",
    "    pred /= ALPHA\n",
    "\n",
    "    #pred = post_process(pred, target)\n",
    "    uncertainty = np.sqrt(2*(np.mean(imgs ** 2, axis=0)) + (np.std(imgs) ** 2))\n",
    "    std = np.sqrt(np.var(imgs, axis=0))\n",
    "    error = np.subtract(pred.flatten(), target.flatten()) \n",
    "\n",
    "    print(\"Total Uncertainty: \", np.sum(uncertainty))\n",
    "    print(\"Total STD: \", np.sum(std))\n",
    "    print(\"Total Error: \", np.sum(error))\n",
    "\n",
    "    print(\"PRED: \", np.sum(pred))\n",
    "    print(\"TARGET: \", np.sum(target))\n",
    "\n",
    "    #obs2, visibility, cphase, camp, F_vis, F_cphase, F_camp, sigma_vis, sigma_cphase, sigma_camp = get_measurements(pred.flatten(), th_noise, amp_err, phase_err, gainp, gain_offset)  # data terms\n",
    "    #plotall(obs2, 'uvdist', 'amp')\n",
    "    #plotall(obs2, 'uvdist', 'phase')\n",
    "\n",
    "    # Compute chi^2 values with observations\n",
    "    pred_img = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    pred_img.imvec = (pred).flatten()\n",
    "    vis_chisq = obs.chisq(pred_img,dtype='vis') \n",
    "    cphase_chisq = obs.chisq(pred_img,dtype='cphase') \n",
    "    camp_chisq = obs.chisq(pred_img, dtype='camp')\n",
    "\n",
    "    if savefile is not None:\n",
    "        target_blur.save_fits(savefile+'_img')\n",
    "\n",
    "    # Compute cross-correlation\n",
    "    recon_xc = compute_xcorr(target, pred/ALPHA)\n",
    "\n",
    "    # Visualize results\n",
    "    result = [target, pred, std, error, vis_chisq, cphase_chisq, camp_chisq, recon_xc]\n",
    "    visualize(result, obs)\n",
    "\n",
    "    return vis_chisq, cphase_chisq, camp_chisq, recon_xc\n",
    "\n",
    "''' Compute Metrics with Pre-Computed Image '''\n",
    "def run_test(target, pred, fwhm, th_noise=False, amp_err=False, phase_err=False, gainp=0.1, gain_offset=0.1, blur_param=0.0):\n",
    "    # Normalize fluxes for EHTIM functions\n",
    "    pred /= np.sum(pred)\n",
    "    target /= np.sum(target)\n",
    "    \n",
    "    print(\"Flux of Target = \", np.sum(target))\n",
    "    print(\"Flux of Pred = \", np.sum(pred))\n",
    "    \n",
    "    nonblur_target = target\n",
    "    \n",
    "    # Blur target by blur_param\n",
    "    target_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    target_blur.imvec = target.flatten()\n",
    "    target_blur = target_blur.blur_circ(fwhm_i=blur_param*fwhm, fwhm_pol=blur_param*fwhm)\n",
    "    target = target_blur.imvec\n",
    "    \n",
    "    obs, visibility, cphase, camp, F_vis, F_cphase, F_camp, sigma_vis, sigma_cphase, sigma_camp, t1, t2 = get_measurements(target.flatten(), th_noise, amp_err, phase_err, gainp, gain_offset)  # data terms\n",
    "    \n",
    "    # Normalize sigma\n",
    "    sigma_vis /= np.sum(sigma_vis)\n",
    "    \n",
    "    # Apply blur to predicted image\n",
    "    fwhm = obs.res()\n",
    "    pred_unblur = pred\n",
    "    pred_blur = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    pred_blur.imvec = pred.flatten()\n",
    "    pred_blur = pred_blur.blur_circ(fwhm_i=blur_param*fwhm, fwhm_pol=blur_param*fwhm)\n",
    "    pred = pred_blur.imvec\n",
    "    \n",
    "    obs_pred, visibility_pred, cphase, camp, F_vis2, F_cphase2, F_camp2, sigma_vis2, sigma_cphase, sigma_camp, t1, t2 = get_measurements(pred.flatten(), th_noise, amp_err, phase_err, gainp, gain_offset)  # data terms\n",
    "    \n",
    "    plt.figure()\n",
    "    x1, uv1, amp1 = plotall(obs, 'uvdist', 'amp', show=False, color='green')\n",
    "    x2, uv2, amp2 = plotall(obs_pred, 'uvdist', 'amp', show=False, color='blue')\n",
    "    plt.plot(uv1, amp1,'o', color='green', markersize=6, label='True Amp.')\n",
    "    plt.plot(uv2, amp2,'o', color='blue', markersize=6, label='Pred Amp.')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    x1, uv1, amp1 = plotall(obs, 'uvdist', 'phase', show=False, color='green')\n",
    "    x2, uv2, amp2 = plotall(obs_pred, 'uvdist', 'phase', show=False, color='blue')\n",
    "    plt.plot(uv1, amp1, 'o', color='green', markersize=6, label='True Phase')\n",
    "    plt.plot(uv2, amp2, 'o', color='blue', markersize=6, label='Pred Phase')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute chi^2 values with observations\n",
    "    pred_img = eh.image.make_empty(npix, fov, ra, dec, rf=rf, source='random', mjd=mjd)\n",
    "    pred_img.imvec = pred.flatten()\n",
    "    vis_chisq = obs.chisq(pred_img,dtype='vis') \n",
    "    #vis_chisq = np.mean(np.abs((visibility_pred-visibility)/sigma_vis)**2)/(2*len(visibility))\n",
    "    cphase_chisq = obs.chisq(pred_img,dtype='cphase') \n",
    "    camp_chisq = obs.chisq(pred_img, dtype='camp')\n",
    "\n",
    "    # Compute cross-correlation, MAE, and error\n",
    "    recon_xc = compute_xcorr(target, pred)\n",
    "    recon_mae = compute_mae(target, pred)\n",
    "    recon_ssim = compute_ssim(target, pred)\n",
    "    error = np.subtract(pred.flatten(), target.flatten())\n",
    "\n",
    "    # Visualize results\n",
    "    result = [nonblur_target, pred_unblur, pred, pred, vis_chisq, cphase_chisq, camp_chisq, recon_mae, recon_ssim]\n",
    "    visualize(result, obs)\n",
    "\n",
    "    return vis_chisq, cphase_chisq, camp_chisq, recon_mae, recon_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot training and validation loss from .txt file (terminal output) '''\n",
    "from itertools import groupby \n",
    "import re  \n",
    "from scipy.optimize import curve_fit\n",
    "    \n",
    "''' Plot training and validation loss from .txt file (terminal output) '''\n",
    "def plot_loss(filename, metric=''):\n",
    "    print(filename)\n",
    "    \n",
    "    history_train = {'total': [], 'xc': [], 'chisq': []}\n",
    "    history_valid = {'total': [], 'xc': [], 'chisq': []}\n",
    "    history_fashion = {'total': [], 'xc': [], 'chisq': []}\n",
    "    history_mnist = {'total': [], 'xc': [], 'chisq': []}\n",
    "    history_bh = {'total': [], 'xc': [], 'chisq': []}\n",
    "    \n",
    "    text = []\n",
    "    i = 0\n",
    "    with open(filename) as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if 'fashion_data' in line:\n",
    "                try:\n",
    "                    [mae, chisq] = re.findall(\"\\\\d+\\\\.*\\\\d*\", line)\n",
    "                    history_fashion['xc'].append(float(mae))\n",
    "                    history_fashion['chisq'].append(float(chisq)/(2*1691))\n",
    "                except: \n",
    "                    print(\"Error line: \", line)\n",
    "                    continue\n",
    "            elif 'mnist_data' in line:\n",
    "                try:\n",
    "                    [mae, chisq] = re.findall(\"\\\\d+\\\\.*\\\\d*\", line)\n",
    "                    history_mnist['xc'].append(float(mae))\n",
    "                    history_mnist['chisq'].append(float(chisq)/(2*1691))\n",
    "                except: \n",
    "                    print(\"Error line: \", line)\n",
    "                    continue\n",
    "            elif 'bh_data' in line:\n",
    "                try:\n",
    "                    [mae, chisq] = re.findall(\"\\\\d+\\\\.*\\\\d*\", line)\n",
    "                    history_bh['xc'].append(float(mae))\n",
    "                    history_bh['chisq'].append(float(chisq)/(2*1691))\n",
    "                except:\n",
    "                    print(\"Error line: \", line)\n",
    "                    continue\n",
    "            elif 'loss' in line:\n",
    "                #print(line)\n",
    "                text.append(line.strip()+ \" - \")\n",
    "\n",
    "    split_lists = np.array([line.split(' - ') for line in text])\n",
    "    split1 = []\n",
    "    for arr in split_lists:\n",
    "        for x in arr:\n",
    "            split1.append(x)\n",
    "    #split1 = np.array([itm for itm in split1 if itm != []])\n",
    "    \n",
    "    for item in split1:\n",
    "        try :\n",
    "            val = re.findall(\"\\\\d+\\\\.*\\\\d*\", item)\n",
    "        except:\n",
    "            continue\n",
    "        if len(val) == 0:\n",
    "            continue\n",
    "        val = float(val[0])\n",
    "        if 'val_xc_mnist_loss' in item:\n",
    "            history_mnist['xc'].append(val)\n",
    "        elif 'val_pred_vis_mnist_loss' in item:\n",
    "            history_mnist['chisq'].append(val)\n",
    "        elif 'val_xc_bh_loss' in item:\n",
    "            history_bh['xc'].append(val)\n",
    "        elif 'val_pred_vis_bh_loss' in item:\n",
    "            history_bh['chisq'].append(val)\n",
    "        elif 'val_xc_loss' in item:\n",
    "            history_valid['xc'].append(val)\n",
    "        elif 'val_pred_vis_loss' in item:\n",
    "            history_valid['chisq'].append(val)\n",
    "        elif 'pred_vis_loss' in item:\n",
    "            history_train['chisq'].append(val)\n",
    "        elif 'xc_loss' in item:\n",
    "            history_train['xc'].append(val)\n",
    "    \n",
    "    # Fit validation curve\n",
    "    def func(x, a, b, c):\n",
    "        return a * x **b + c\n",
    "    \n",
    "    cutoff = 2\n",
    "    xpoints = range(len(history_valid['xc'][cutoff:]))\n",
    "    ypoints = history_valid['xc'][cutoff:]\n",
    "    popt, pcov = curve_fit(func, xpoints, np.log10(ypoints))\n",
    "    \n",
    "    # Compute total loss\n",
    "    train_total_loss = [(history_train['xc'][i] + history_train['chisq'][i]) for i in range(len(history_train['xc']))]             \n",
    "    train_total_loss = [elem for elem in train_total_loss if elem < 10]\n",
    "    valid_total_loss = [(history_valid['xc'][i] + history_valid['chisq'][i]) for i in range(len(history_valid['xc']))]   \n",
    "    valid_total_loss = [elem for elem in valid_total_loss if elem < 10]\n",
    "    fashion_total_loss = [(history_fashion['xc'][i] + history_fashion['chisq'][i]) for i in range(len(history_fashion['xc']))]   \n",
    "   # fashion_total_loss = [elem for elem in fashion_total_loss if elem < 10]\n",
    "    mnist_total_loss = [(history_mnist['xc'][i] + history_mnist['chisq'][i]) for i in range(len(history_mnist['xc']))]   \n",
    "   # mnist_total_loss = [elem for elem in mnist_total_loss if elem < 10]\n",
    "    bh_total_loss = [(history_bh['xc'][i] + history_bh['chisq'][i]) for i in range(len(history_bh['xc']))]   \n",
    "   # bh_total_loss = [elem for elem in bh_total_loss if elem < 10]\n",
    "    \n",
    "    #history_train['xc'] = [x[0] for x in groupby(history_train['xc'])]\n",
    "    #history_train['chisq'] = [x[0] for x in groupby(history_train['chisq'])]\n",
    "    #train_total_loss = [x[0] for x in groupby(train_total_loss)]\n",
    "    \n",
    "    # Plot total loss\n",
    "    plt.figure()\n",
    "    #plt.plot(np.log10(train_total_loss[cutoff:]), label='Train Loss ')\n",
    "    #plt.plot(np.log10(valid_total_loss[cutoff:]), label='Valid Loss ')\n",
    "    #plt.plot(xpoints, func(xpoints, *popt), label='Validation Fit')\n",
    "    plt.plot(np.log10(fashion_total_loss[cutoff:]), label='Fashion Loss ')\n",
    "    plt.plot(np.log10(mnist_total_loss[cutoff:]), label='MNIST Loss ')\n",
    "    plt.plot(np.log10(bh_total_loss[cutoff:]), label='Black Hole Loss ')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log(Total Loss)')\n",
    "    plt.title('Log(Total Loss)'+' vs. Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.figure()\n",
    "    #plt.plot(np.log10(history_train['xc'][cutoff:]), label='Train MAE ')\n",
    "    #plt.plot(np.log10(history_valid['xc'][cutoff:]), label='Valid MAE ')\n",
    "    #plt.plot(xpoints, func(xpoints, *popt), label='Validation Fit')\n",
    "    plt.plot(np.log10(history_fashion['xc'][:]), label='Fashion MAE '+metric)\n",
    "    plt.plot(np.log10(history_mnist['xc'][:]), label='MNIST MAE '+metric)\n",
    "    plt.plot(np.log10(history_bh['xc'][:]), label='BH MAE '+metric)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log(MAE)')\n",
    "    plt.title('Log(MAE)'+' vs. Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    min_loss = min(history_valid['xc'])\n",
    "    print(\"Min Validation MAE = \", min_loss, \" at epoch \", list(history_valid['xc']).index(min_loss))\n",
    "    min_loss = min(history_mnist['xc'])\n",
    "    print(\"Min MNIST MAE = \", min_loss, \" at epoch \", list(history_mnist['xc']).index(min_loss))\n",
    "    min_loss = min(history_bh['xc'])\n",
    "    print(\"Min BH MAE = \", min_loss, \" at epoch \", list(history_bh['xc']).index(min_loss))\n",
    "    \n",
    "    xpoints = range(len(history_valid['chisq'][cutoff:]))\n",
    "    ypoints = history_valid['chisq'][cutoff:]\n",
    "    popt, pcov = curve_fit(func, xpoints, np.log10(ypoints))\n",
    "    \n",
    "    plt.figure()\n",
    "    #plt.plot(np.log10(history_train['chisq'][cutoff:]), label='Train Chi^2 ')\n",
    "    #plt.plot(np.log10(history_valid['chisq'][cutoff:]), label='Valid Chi^2 ')\n",
    "    #plt.plot(xpoints, func(xpoints, *popt), label='Validation Fit')\n",
    "    plt.plot(np.log10(history_fashion['chisq'][:]), label='Fashion Chi^2 '+metric)\n",
    "    plt.plot(np.log10(history_mnist['chisq'][:]), label='MNIST Chi^2 '+metric)\n",
    "    plt.plot(np.log10(history_bh['chisq'][:]), label='BH Chi^2 '+metric)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log(Chi^2)')\n",
    "    plt.title('Log(Chi^2)'+' vs. Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    min_loss = min(history_valid['chisq'])\n",
    "    print(\"Min Validation Chi^2 = \", min_loss, \" at epoch \", list(history_valid['chisq']).index(min_loss))\n",
    "    min_loss = min(history_mnist['chisq'])\n",
    "    print(\"Min MNIST Chi^2 = \", min_loss, \" at epoch \", list(history_mnist['chisq']).index(min_loss))\n",
    "    min_loss = min(history_bh['chisq'])\n",
    "    print(\"Min BH Chi^2 = \", min_loss, \" at epoch \", list(history_bh['chisq']).index(min_loss))\n",
    "    \n",
    "    #plt.plot(np.log10(history_mnist['chisq'][1:]), label='MNIST CHI^2 '+metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and select target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print('Loading dataset...')\n",
    "dataset = 'bh_data'\n",
    "xdata = get_data(dataset)\n",
    "\n",
    "# Select target image at index 0\n",
    "X = xdata[0]\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate observations from target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating empty observation file . . . \n",
      "Producing clean visibilities from image with nfft FT . . . \n",
      "Adding gain + phase errors to data and applying a priori calibration . . . \n",
      "Finished computing visibilities...\n",
      "Getting bispectra:: type vis, count max, scan 106/106 \n",
      "\n",
      "Updated self.cphase: no averaging\n",
      "updated self.cphase: avg_time 0.000000 s\n",
      "\n",
      "Getting closure amps:: type vis camp , count max, scan 106/106\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "obs, visibility, cphase, camp, F_vis, F_cphase, F_camp, sigma_vis, sigma_cphase, sigma_camp, t1, t2 = get_measurements(X.flatten())  # data terms\n",
    "fwhm = obs.res()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test sample from dataset\n",
    "pred_img_path = '/path/to/predicted/image'\n",
    "pred = np.load(pred_img_path)\n",
    "vis_chisq, cphase_chisq, camp_chisq, mae, ssim = run_test(target, pred, fwhm, blur_param=0.5)\n",
    "\n",
    "print('Visibility Chi^2 = ', vis_chisq)\n",
    "print('Closure Phase Chi^2 = ', cphase_chisq)\n",
    "print('MAE = ', mae)\n",
    "print('SSIM = ', ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
